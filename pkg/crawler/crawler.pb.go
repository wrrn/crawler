// Code generated by protoc-gen-go. DO NOT EDIT.
// source: crawler.proto

package crawler // import "github.com/wrrn/crawler/pkg/crawler"

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"

import (
	context "golang.org/x/net/context"
	grpc "google.golang.org/grpc"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

// StartRequest is sent to the service to indicate the URL it should start crawling.
type StartRequest struct {
	Url                  string   `protobuf:"bytes,1,opt,name=url" json:"url,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StartRequest) Reset()         { *m = StartRequest{} }
func (m *StartRequest) String() string { return proto.CompactTextString(m) }
func (*StartRequest) ProtoMessage()    {}
func (*StartRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{0}
}
func (m *StartRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StartRequest.Unmarshal(m, b)
}
func (m *StartRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StartRequest.Marshal(b, m, deterministic)
}
func (dst *StartRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StartRequest.Merge(dst, src)
}
func (m *StartRequest) XXX_Size() int {
	return xxx_messageInfo_StartRequest.Size(m)
}
func (m *StartRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_StartRequest.DiscardUnknown(m)
}

var xxx_messageInfo_StartRequest proto.InternalMessageInfo

func (m *StartRequest) GetUrl() string {
	if m != nil {
		return m.Url
	}
	return ""
}

// StartResponse indicates a success, but has no fields.
type StartResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StartResponse) Reset()         { *m = StartResponse{} }
func (m *StartResponse) String() string { return proto.CompactTextString(m) }
func (*StartResponse) ProtoMessage()    {}
func (*StartResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{1}
}
func (m *StartResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StartResponse.Unmarshal(m, b)
}
func (m *StartResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StartResponse.Marshal(b, m, deterministic)
}
func (dst *StartResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StartResponse.Merge(dst, src)
}
func (m *StartResponse) XXX_Size() int {
	return xxx_messageInfo_StartResponse.Size(m)
}
func (m *StartResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_StartResponse.DiscardUnknown(m)
}

var xxx_messageInfo_StartResponse proto.InternalMessageInfo

// StopRequest is sent to the service to indicate which URL it should stop crawling.
type StopRequest struct {
	Url                  string   `protobuf:"bytes,1,opt,name=url" json:"url,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StopRequest) Reset()         { *m = StopRequest{} }
func (m *StopRequest) String() string { return proto.CompactTextString(m) }
func (*StopRequest) ProtoMessage()    {}
func (*StopRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{2}
}
func (m *StopRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StopRequest.Unmarshal(m, b)
}
func (m *StopRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StopRequest.Marshal(b, m, deterministic)
}
func (dst *StopRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StopRequest.Merge(dst, src)
}
func (m *StopRequest) XXX_Size() int {
	return xxx_messageInfo_StopRequest.Size(m)
}
func (m *StopRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_StopRequest.DiscardUnknown(m)
}

var xxx_messageInfo_StopRequest proto.InternalMessageInfo

func (m *StopRequest) GetUrl() string {
	if m != nil {
		return m.Url
	}
	return ""
}

// StartResponse indicates a success, but has no fields.
type StopResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StopResponse) Reset()         { *m = StopResponse{} }
func (m *StopResponse) String() string { return proto.CompactTextString(m) }
func (*StopResponse) ProtoMessage()    {}
func (*StopResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{3}
}
func (m *StopResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StopResponse.Unmarshal(m, b)
}
func (m *StopResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StopResponse.Marshal(b, m, deterministic)
}
func (dst *StopResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StopResponse.Merge(dst, src)
}
func (m *StopResponse) XXX_Size() int {
	return xxx_messageInfo_StopResponse.Size(m)
}
func (m *StopResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_StopResponse.DiscardUnknown(m)
}

var xxx_messageInfo_StopResponse proto.InternalMessageInfo

// ListRequest tells the service to return the "site tree" for the all the crawled URLs.
type ListRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ListRequest) Reset()         { *m = ListRequest{} }
func (m *ListRequest) String() string { return proto.CompactTextString(m) }
func (*ListRequest) ProtoMessage()    {}
func (*ListRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{4}
}
func (m *ListRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ListRequest.Unmarshal(m, b)
}
func (m *ListRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ListRequest.Marshal(b, m, deterministic)
}
func (dst *ListRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ListRequest.Merge(dst, src)
}
func (m *ListRequest) XXX_Size() int {
	return xxx_messageInfo_ListRequest.Size(m)
}
func (m *ListRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ListRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ListRequest proto.InternalMessageInfo

// ListResponse contains the "site trees" for all of the crawled URLs.
type ListResponse struct {
	SiteTrees            []*SiteTree `protobuf:"bytes,1,rep,name=site_trees,json=siteTrees" json:"site_trees,omitempty"`
	XXX_NoUnkeyedLiteral struct{}    `json:"-"`
	XXX_unrecognized     []byte      `json:"-"`
	XXX_sizecache        int32       `json:"-"`
}

func (m *ListResponse) Reset()         { *m = ListResponse{} }
func (m *ListResponse) String() string { return proto.CompactTextString(m) }
func (*ListResponse) ProtoMessage()    {}
func (*ListResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{5}
}
func (m *ListResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ListResponse.Unmarshal(m, b)
}
func (m *ListResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ListResponse.Marshal(b, m, deterministic)
}
func (dst *ListResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ListResponse.Merge(dst, src)
}
func (m *ListResponse) XXX_Size() int {
	return xxx_messageInfo_ListResponse.Size(m)
}
func (m *ListResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ListResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ListResponse proto.InternalMessageInfo

func (m *ListResponse) GetSiteTrees() []*SiteTree {
	if m != nil {
		return m.SiteTrees
	}
	return nil
}

// SiteTree represents a single url's site tree.
type SiteTree struct {
	Url                  string   `protobuf:"bytes,1,opt,name=url" json:"url,omitempty"`
	Tree                 []*Tree  `protobuf:"bytes,2,rep,name=tree" json:"tree,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SiteTree) Reset()         { *m = SiteTree{} }
func (m *SiteTree) String() string { return proto.CompactTextString(m) }
func (*SiteTree) ProtoMessage()    {}
func (*SiteTree) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{6}
}
func (m *SiteTree) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SiteTree.Unmarshal(m, b)
}
func (m *SiteTree) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SiteTree.Marshal(b, m, deterministic)
}
func (dst *SiteTree) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SiteTree.Merge(dst, src)
}
func (m *SiteTree) XXX_Size() int {
	return xxx_messageInfo_SiteTree.Size(m)
}
func (m *SiteTree) XXX_DiscardUnknown() {
	xxx_messageInfo_SiteTree.DiscardUnknown(m)
}

var xxx_messageInfo_SiteTree proto.InternalMessageInfo

func (m *SiteTree) GetUrl() string {
	if m != nil {
		return m.Url
	}
	return ""
}

func (m *SiteTree) GetTree() []*Tree {
	if m != nil {
		return m.Tree
	}
	return nil
}

// Tree represents a site's directory tree. A tree that does not have children is considered a leaf node.
type Tree struct {
	Name                 string   `protobuf:"bytes,1,opt,name=name" json:"name,omitempty"`
	Children             []*Tree  `protobuf:"bytes,2,rep,name=children" json:"children,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *Tree) Reset()         { *m = Tree{} }
func (m *Tree) String() string { return proto.CompactTextString(m) }
func (*Tree) ProtoMessage()    {}
func (*Tree) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawler_1a873397de1f4d0e, []int{7}
}
func (m *Tree) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_Tree.Unmarshal(m, b)
}
func (m *Tree) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_Tree.Marshal(b, m, deterministic)
}
func (dst *Tree) XXX_Merge(src proto.Message) {
	xxx_messageInfo_Tree.Merge(dst, src)
}
func (m *Tree) XXX_Size() int {
	return xxx_messageInfo_Tree.Size(m)
}
func (m *Tree) XXX_DiscardUnknown() {
	xxx_messageInfo_Tree.DiscardUnknown(m)
}

var xxx_messageInfo_Tree proto.InternalMessageInfo

func (m *Tree) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *Tree) GetChildren() []*Tree {
	if m != nil {
		return m.Children
	}
	return nil
}

func init() {
	proto.RegisterType((*StartRequest)(nil), "crawler.v1.StartRequest")
	proto.RegisterType((*StartResponse)(nil), "crawler.v1.StartResponse")
	proto.RegisterType((*StopRequest)(nil), "crawler.v1.StopRequest")
	proto.RegisterType((*StopResponse)(nil), "crawler.v1.StopResponse")
	proto.RegisterType((*ListRequest)(nil), "crawler.v1.ListRequest")
	proto.RegisterType((*ListResponse)(nil), "crawler.v1.ListResponse")
	proto.RegisterType((*SiteTree)(nil), "crawler.v1.SiteTree")
	proto.RegisterType((*Tree)(nil), "crawler.v1.Tree")
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// Client API for Crawler service

type CrawlerClient interface {
	// Start signals the service to start crawling the given URL.
	Start(ctx context.Context, in *StartRequest, opts ...grpc.CallOption) (*StartResponse, error)
	// Stop signals the service to stop crawling the given URL.
	Stop(ctx context.Context, in *StopRequest, opts ...grpc.CallOption) (*StopResponse, error)
	// Show the current site tree for all the given URLs.
	List(ctx context.Context, in *ListRequest, opts ...grpc.CallOption) (*ListResponse, error)
}

type crawlerClient struct {
	cc *grpc.ClientConn
}

func NewCrawlerClient(cc *grpc.ClientConn) CrawlerClient {
	return &crawlerClient{cc}
}

func (c *crawlerClient) Start(ctx context.Context, in *StartRequest, opts ...grpc.CallOption) (*StartResponse, error) {
	out := new(StartResponse)
	err := grpc.Invoke(ctx, "/crawler.v1.Crawler/Start", in, out, c.cc, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *crawlerClient) Stop(ctx context.Context, in *StopRequest, opts ...grpc.CallOption) (*StopResponse, error) {
	out := new(StopResponse)
	err := grpc.Invoke(ctx, "/crawler.v1.Crawler/Stop", in, out, c.cc, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *crawlerClient) List(ctx context.Context, in *ListRequest, opts ...grpc.CallOption) (*ListResponse, error) {
	out := new(ListResponse)
	err := grpc.Invoke(ctx, "/crawler.v1.Crawler/List", in, out, c.cc, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// Server API for Crawler service

type CrawlerServer interface {
	// Start signals the service to start crawling the given URL.
	Start(context.Context, *StartRequest) (*StartResponse, error)
	// Stop signals the service to stop crawling the given URL.
	Stop(context.Context, *StopRequest) (*StopResponse, error)
	// Show the current site tree for all the given URLs.
	List(context.Context, *ListRequest) (*ListResponse, error)
}

func RegisterCrawlerServer(s *grpc.Server, srv CrawlerServer) {
	s.RegisterService(&_Crawler_serviceDesc, srv)
}

func _Crawler_Start_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(StartRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(CrawlerServer).Start(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/crawler.v1.Crawler/Start",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(CrawlerServer).Start(ctx, req.(*StartRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _Crawler_Stop_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(StopRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(CrawlerServer).Stop(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/crawler.v1.Crawler/Stop",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(CrawlerServer).Stop(ctx, req.(*StopRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _Crawler_List_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ListRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(CrawlerServer).List(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/crawler.v1.Crawler/List",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(CrawlerServer).List(ctx, req.(*ListRequest))
	}
	return interceptor(ctx, in, info, handler)
}

var _Crawler_serviceDesc = grpc.ServiceDesc{
	ServiceName: "crawler.v1.Crawler",
	HandlerType: (*CrawlerServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Start",
			Handler:    _Crawler_Start_Handler,
		},
		{
			MethodName: "Stop",
			Handler:    _Crawler_Stop_Handler,
		},
		{
			MethodName: "List",
			Handler:    _Crawler_List_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "crawler.proto",
}

func init() { proto.RegisterFile("crawler.proto", fileDescriptor_crawler_1a873397de1f4d0e) }

var fileDescriptor_crawler_1a873397de1f4d0e = []byte{
	// 304 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x74, 0x52, 0xcf, 0x4f, 0x83, 0x30,
	0x14, 0x1e, 0x0e, 0x75, 0x7b, 0x1b, 0xba, 0x34, 0x26, 0xe2, 0x2e, 0x92, 0xaa, 0xc9, 0x0e, 0x06,
	0xe2, 0x76, 0x34, 0xf1, 0xb0, 0x5d, 0x3c, 0x78, 0x02, 0x4f, 0x5e, 0x0c, 0xc3, 0x97, 0x8d, 0xc8,
	0x00, 0xdb, 0xe2, 0xfe, 0x40, 0xff, 0x31, 0xd3, 0x52, 0x66, 0xc9, 0xdc, 0xed, 0x7b, 0x7c, 0x3f,
	0x78, 0xfd, 0x5a, 0x70, 0x12, 0x16, 0x6f, 0x33, 0x64, 0x7e, 0xc9, 0x0a, 0x51, 0x10, 0x68, 0xc6,
	0xef, 0x07, 0xea, 0xc1, 0x30, 0x12, 0x31, 0x13, 0x21, 0x7e, 0x55, 0xc8, 0x05, 0x19, 0x41, 0xb7,
	0x62, 0x99, 0x6b, 0x79, 0xd6, 0xa4, 0x1f, 0x4a, 0x48, 0xcf, 0xc1, 0xd1, 0x0a, 0x5e, 0x16, 0x39,
	0x47, 0x7a, 0x0d, 0x83, 0x48, 0x14, 0xe5, 0x61, 0xc7, 0x99, 0xcc, 0x94, 0x02, 0x6d, 0x70, 0x60,
	0xf0, 0x92, 0xf2, 0xe6, 0x17, 0x74, 0x01, 0xc3, 0x7a, 0xac, 0x69, 0x32, 0x03, 0xe0, 0xa9, 0xc0,
	0x77, 0xc1, 0x10, 0xb9, 0x6b, 0x79, 0xdd, 0xc9, 0x60, 0x7a, 0xe1, 0xff, 0xed, 0xe8, 0x47, 0xa9,
	0xc0, 0x57, 0x86, 0x18, 0xf6, 0xb9, 0x46, 0x9c, 0xce, 0xa1, 0xd7, 0x7c, 0xde, 0xdf, 0x80, 0xdc,
	0x82, 0x2d, 0xd3, 0xdc, 0x23, 0x15, 0x36, 0x32, 0xc3, 0x54, 0x90, 0x62, 0xe9, 0x33, 0xd8, 0xca,
	0x4f, 0xc0, 0xce, 0xe3, 0x0d, 0xea, 0x00, 0x85, 0xc9, 0x3d, 0xf4, 0x92, 0x75, 0x9a, 0x7d, 0x30,
	0xcc, 0x0f, 0xa6, 0xec, 0x14, 0xd3, 0x1f, 0x0b, 0x4e, 0x17, 0x35, 0x4b, 0x9e, 0xe0, 0x58, 0xf5,
	0x45, 0xdc, 0xd6, 0x19, 0x8c, 0x92, 0xc7, 0x57, 0xff, 0x30, 0xba, 0xab, 0x0e, 0x79, 0x04, 0x5b,
	0xb6, 0x47, 0x2e, 0xdb, 0xa2, 0x5d, 0xe1, 0x63, 0x77, 0x9f, 0x30, 0xcd, 0xb2, 0xdb, 0xb6, 0xd9,
	0x28, 0xbf, 0x6d, 0x36, 0xaf, 0x81, 0x76, 0xe6, 0x77, 0x6f, 0x37, 0xab, 0x54, 0xac, 0xab, 0xa5,
	0x9f, 0x14, 0x9b, 0x60, 0xcb, 0x58, 0x1e, 0x68, 0x71, 0x50, 0x7e, 0xae, 0x1a, 0xbc, 0x3c, 0x51,
	0xaf, 0x68, 0xf6, 0x1b, 0x00, 0x00, 0xff, 0xff, 0x62, 0xd6, 0x06, 0x0b, 0x56, 0x02, 0x00, 0x00,
}
